---
title: "Bayesian Statistics"
subtitle: "A Conceptual and Practical Introduction"
author: "Fabian Dablander"
date: 25th April, 2017
runtime: shiny
output:
  ioslides_presentation:
    css: styles.css
    smaller: yes
    widescreen: yes
    transition: faster
---

```{r setup, include=FALSE, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, dev.args = list(bg = 'transparent'), fig.align = 'center')
```

\usepackages{csquotes}
\newcommand{\param}{\mathbf{\theta}}
\newcommand{\dat}{\textbf{y}}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

## Motivation
![](images/statistical-tests.png)

## Learning outcomes
- **Oblique**
    - Get a feeling for *statistical modeling*
    - Get a feeling for statistics as a (young) discipline
- **Concrete**
    - Be able to state the differences between classical and Bayesian statistics
    - Be able to apply simple Bayesian models to your data
- **Wishful**
    - Become hungry for statistical developments
    - Read the linked resources at https://github.com/fdabl/Intro-Stats to dive deeper


## Outline
- **1a.** History of statistics
    - Controversies and enigmatic key players
    - The superego, ego, and the id in statistical reasoning
    
<span style = "color:white"></span>

- **1b.** Introduction to probability
    - Why most published research is false
    - Do I have testicular cancer?
    - Frequentist versus Bayesian interpretation of probability

<span style = "color:white"></span>
    
- **2.** Foundation: Statistical models
    - Bread and butter example
    - Likelihoods, law of large number, central limit theorem
    
<span style = "color:white"></span>
    
- **3.** Classical statistics
    - Maximum likelihood, sampling distributions, bootstrapping
    - Confidence intervals, $p$-values

## Outline
- **4.** Introduction to Bayesian statistics
    - Parameter estimation
    - Model comparison
    - Model prediction
    
<span style = "color:white"></span>

- **5.** Some applications of Bayesian principles
    - Simulator sickness and age
    - Harry Potter and Personality
    - Markov chain Monte Carlo with People
    
<span style = "color:white"></span>

- **6.** Practical examples using JASP
    - Is height associated with being president?
    - *~ ~ tralala ~ ~ ... Wish we could turn back time .. ~ ~ tralala ~ ~*
    
<span style = "color:white"></span>
    
- **7.** Final thoughts and further resources

# History of statistics

## Overview
- Before 1940, significance testing was virtually non-existent
- By 1955, 80% of empirical articles reported significance tests
- Today, the figure is close to 100% (see Gigerenzer, 1993)

<span style="color: white;"></span>

<span style = "font-size: 1.4em; line-height: 120%; margin-top: -800px;">
"[Statisticians] have already overrun every branch of science with a rapidity of conquest rivalled only by Atilla, Mohammed, and the Colorado beetle."

- Kendall (1942, p.69)
</span>


## History
- Fienberg (1992) identifies four periods:
- **1660 - 1750**: Pascal, Fermat, Bernoulli, De Moivre
- **1750 - 1820**: Bayes, Laplace, Legendre, Gauss
- **1820 - 1900**: Quetelet, Galton, Edgeworth, Yule, K. Pearson
- **1900 - 1950**: Gosset, Fisher, Neyman, E. Pearson, Ramsey, Kolmogorov, de Finetti, Jeffreys, Wald, Savage


## Ronald Fisher
<div style = "float:left; width:40%;">
![](images/fisher.png)
</div>
<div style = "float:right; width:55%;">
- Father of modern statistical inference
- Randomization, design of experiments
- $p$-value, $\alpha$ level, null hypothesis, analysis of variance
- Maximum likelihood, sufficient statistics
- Parametric modeling, distributional theory
- Founder of population genetics
- Sexy son hypothesis
</div>


## Fisher's approach to testing
1) Setup a null hypothesis.
2) Report the exact level of significance (e.g., $p = .051$ or $p = .049$). Do not use a conventional 5% level, and
do not talk about accepting or rejecting hypotheses.
3) Use this procedure only if you know very little about the problem at hand


## Ronald Fisher: Evidential statistics
<span style = "font-size: 1.5em; line-height: 120%; margin-top: -800px;">
"... no scientific worker has a fixed level of significance at which from year to year, and in all
circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light of his evidence and his ideas."

- Sir Ronald A. Fisher (1956)
</span>


## Neyman-Pearson
- Jerzy Neyman (1894 - 1981) was a polish man who did much of his work in the United States
- Egon Pearson (1895 - 1980) was the son of the eminent Karl Pearson
- Together, they put Fisher's ideas on a rigorous mathematical basis
- Fisher didn't like it, and a personal feud began that lasted until his death

<span style = "color: white;"></span>

- Some concepts they introduced were
    - The alternative hypothesis
    - Type I ($\alpha$) and Type II ($\beta$) errors
    - Statistical power
    - Confidence intervals
    - Decision theoretic foundation of hypothesis testing


## Neyman-Pearson's approach to testing
1) Setup two statistical hypotheses, $H_1$ and $H_2$, and decide about $\alpha$, $\beta$, and sample size before the experiment, based on subjective cost-benefit considerations. These define a rejection region for each hypothesis.

2) If the data falls into the rejection region of $H_1$, accept $H_2$; otherwise accept $H_1$. Note that accepting a hypothesis does not mean that you believe in it, but only that you act is if it were true. Report only $p < \alpha$ or $p \geq \alpha$.

3) The usefulness of the procedure is limited among others to situations where you have a disjunction of hypotheses (e.g., either $\mu_1 = 8$ or $\mu_2 = 10$ is true) and where you can make meaningful cost-benefit trade-offs for choosing $\alpha$ and $\beta$.


## Neyman-Pearson: Behavioural statistics
<span style = "font-size: 1.5em; line-height: 120%; margin-top: -500px;">
"We are inclined to think that as far as a particular hypothesis is concerned, no test based upon the theory of probability can by itself provide any valuable evidence of the truth or falsehood of that hypothesis. But we may look at the purpose of tests from another viewpoint. Without hoping to know whether each separate hypothesis is true or false, we may search for rules to govern our behaviour with regard to them, in following which we insure that, in the long run of experience, we shall not be too often wrong".

- Jerzy Neyman & Egon Pearson (1933, pp. 290-291, as quoted in Johansson, 2011)
</span>


## Current statistical practice
- Is a hybrid between the approaches of Fisher and Neyman-Pearson
- Gigerenzer (2004) introduces the term ''null ritual'' to denote current practices

<span style = "color:white"></span>

    1) Setup a statistical null hypothesis, but do not specifiy your own hypothesis nor any alternative
    2) use the 5% significance level for rejecting the null and accepting your hypothesis
    3) always perform this procedure
    
    
## Current statistical practice
- This is in part due to considerable misunderstandings of statistics
- **[Discuss results of the pre-workshop questionnaire here]**

<span style = "color:white"></span>

- See Oakes, 1986; Haller & Krauss, 2002; Hoekstra et al., 2014


## Bayes-Laplace-Jeffreys
- Thomas Bayes (1701 - 1761) was a Presbyterian minister whose "An essay towards solving a Problem in the Doctrine of Chances" used inverse probability --- Bayes' rule

<span style = "color:white"></span>

- Pierre-Simon Laplace was "the Newton of France"
    - Among many other things, generalized Bayes' rule and proved the central limit theorem

<span style = "color:white"></span>

- Harold Jeffreys (1891 - 1989) revived the Bayesian view of probability
    - Developed the Bayes factor, default Bayesian tests, and Jeffreys priors
    
<span style = "color:white"></span>

- Use Bayes' rule to make uncertainty statements about parameters, models, and hypotheses
- (This is not possible in any other statistical approach)


## Jeffreys' approach to testing
- Jeffreys had four convictions
    - Inference is inductive
    - Induction requires a logic of partial belief
    - The test of a general law requires it be given a prior probability
    - Classical tests are inadequate
    
- (For an excellent account, see Ly, Verhagen, Wagenmakers, 2016)


##
<center>
  <img src="images/super-ego-id.png" />
</center>


# Introduction to Probability

## Probability theory
- Is based on set theory (Kolmogorov, 1933)
- $\mathcal{S} = \{0, 1, 2, 3\}$ denotes the *sample space*
- $\mathcal{A} = \{0, 1\}$ and $\mathcal{B} = \{1, 2\}$ denote events; $\mathcal{A}, \mathcal{B} \subset \mathcal{S}$

$$
\begin{align*}
\mathcal{A^c} &= \{2, 3\} \\
\mathcal{B^c} &= \{0, 3\} \\
\mathcal{A} \cap \mathcal{B} &= \{1\} \\
\mathcal{A} \cup \mathcal{B} &= \{0, 1, 2\}
\end{align*}
$$

## Naive definition
- Naive view assumes equally likely outcomes, resulting in 

$$
P(A) = \frac{|A|}{|S|}
$$

- where $P(.)$ is a function that takes a set and returns a real number on the interval [0, 1]
- In order to be a valid probability function, the function must satisfy certain constraints


## Non-naive definition
- Kolmogorov's axioms (1933)

$$
\begin{align*}
&P(\mathcal{S}) = 1, \,  P(\emptyset) = 0 \\
&P(\bigcup^{\infty}_{i = 1}\mathcal{A}_i) = \sum^{\infty}_{i=1}P(\mathcal{A}_i) \qquad A_1, A_2, \ldots \text{are disjoint}
\end{align*}
$$
<center>
<img src="images/venn-diagram.png" </img>
</center>

## Non-naive definition
- Some neat properties result

$$
\begin{split}
P(\mathcal{A}^c) &= 1 - P(\mathcal{A}^c) \\
P(\mathcal{A \cup B}) &= P(\mathcal{A}) + P(\mathcal{B}) - P(\mathcal{A \cap B}) \\
P(\mathcal{A}) &\leq P(\mathcal{B}) \, \, \, \, \, \, \, \text{  if   } \mathcal{A} \subseteq \mathcal{B}
\end{split}
$$

## How likely is today somebody's birthday?
- There are $k = 35$ people in the room
- We assume that no people share birthdays, i.e., sampling without replacement ($k \leq 365$)
- Then

$$
P(\text{somebody's birthday}) = k \cdot \frac{1}{365}
$$

```{r}
k <- 35
S <- seq(365)
today <- 31 + 28 + 31 + 25

mean(
  replicate(100000, any(sample(S, k, replace = FALSE) == today))
)
```

## How likely is today somebody's birthday?
- Used the law of large numbers to approximate the result (more later)
- But clearly our assumption that people don't share birthdays is wrong

<span style = "color:white"></span>

- How likely is it that at least two people share birthdays?
- Use $P(A^c) = 1 - P(A)$ to simplify the problem


## Birthday Problem
$$
\begin{split}
P(\text{two or more people share birthdays}) &= 1 - P(\text{nobody shares birthdays}) \\[1ex]
                                             &= 1 - \frac{365 \cdot (365 - 1) \cdot \ldots \cdot (365 - k + 1)}{365^k}
\end{split}
$$

```{r, echo = FALSE, fig.width = 6, fig.height = 4}
library('ggplot2')
theme_set(papaja::theme_apa())

k <- seq(0, 50)
f <- function(k) 1 - ((choose(365, k) * factorial(k)) / 365^k)
d <- data.frame(k = k, prob = f(k))

ggplot(d, aes(x = k, y = prob)) +
  geom_line() +
  ylab('Probability') +
  geom_hline(yintercept = .5, linetype = 'dashed')
```


## A murder mystery
- A scream wakes you
- You run in the kitchen and see a dead body --- who killed your husband?

<span style = "color:white"></span>

- There are three possible perpetrators
    - Your husband himself
    - The person he had an affair with
    - The butler

<span style = "color:white"></span>

- There are three possible murder weapons
    - An axe
    - A knife
    - A pistol
    
- How should you proceed?
- Instinctively, you sketch a probability table

## A murder mystery
+-----------------------+-------------------------------+--------------------------+-----------------------------+
|                       |$\text{He killed himself}$     |  $\text{She killed him}$ | $\text{Butler killed him}$  |
+-----------------------+-------------------------------+--------------------------+-----------------------------+
| $\text{Used axe}$     |  $\frac{1}{12}$               | $\frac{1}{12}$           | $\frac{2}{12}$              |
+-----------------------+-------------------------------+--------------------------+-----------------------------+
| $\text{Used knife}$   |  $\frac{1}{12}$               | $\frac{3}{12}$           | $\frac{1}{12}$              |
+-----------------------+-------------------------------+--------------------------+-----------------------------+
| $\text{Used pistol}$  |  $\frac{2}{12}$               | $0$                      | $\frac{1}{12}$              |
+-----------------------+-------------------------------+--------------------------+-----------------------------+
| $\text{Sum (Prior)}$  |  $\frac{1}{3}$                | $\frac{1}{3}$            | $\frac{1}{3}$               |
+-----------------------+-------------------------------+--------------------------+-----------------------------+


## Two types of probability
- **Joint probability**

$$
P(\text{Axe}, \text{Suicide}) = \frac{1}{12}
$$
- **Marginal probability**

$$
\begin{split}
P(\text{Suicide}) &= \frac{1}{3} \\[1ex]
                            &= P(\text{Axe}, \text{Suicide}) +  P(\text{Knife}, \text{Suicide}) + P(\text{Pistol}, \text{Suicide}) \\[1ex]
                            &= \frac{1}{12} + \frac{1}{12} + \frac{2}{12} \\[2ex]
                            &\Rightarrow \sum_{W}^{3} P(W, \text{Suicide}) \qquad \text{Sum rule}
\end{split}
$$


## A murder mystery
- You estimated that each person was equally likely to be the perpetrator
- Now the police has indentified the murder weapon --- an axe, of course
- How does that change your beliefs?
- Who is most likely to have killed your husband?


## Model specification
+-----------------------+-------------------------------+--------------------------+-----------------------------+
|                       |$\text{He killed himself}$     |  $\text{She killed him}$ | $\text{Butler killed him}$  |
+-----------------------+-------------------------------+--------------------------+-----------------------------+
| $\text{Used axe}$     |  $\frac{1}{12}$               | $\frac{1}{12}$           | $\frac{2}{12}$              |
+-----------------------+-------------------------------+--------------------------+-----------------------------+
| $\text{Used knife}$   |  $\frac{1}{12}$               | $\frac{3}{12}$           | $\frac{1}{12}$              |
+-----------------------+-------------------------------+--------------------------+-----------------------------+
| $\text{Used pistol}$  |  $\frac{2}{12}$               | $0$                      | $\frac{1}{12}$              |
+-----------------------+-------------------------------+--------------------------+-----------------------------+
| $\text{Sum (Prior)}$  |  $\frac{1}{3}$                | $\frac{1}{3}$            | $\frac{1}{3}$               |
+-----------------------+-------------------------------+--------------------------+-----------------------------+


## It was an axe! --- data arrive
+-----------------------+-------------------------------+--------------------------+-----------------------------+
|                       |$\text{He killed himself}$     |  $\text{She killed him}$ | $\text{Butler killed him}$  |
+-----------------------+-------------------------------+--------------------------+-----------------------------+
| $\text{Used axe}$     |  $\frac{1}{12}$               | $\frac{1}{12}$           | $\frac{2}{12}$              |
+-----------------------+-------------------------------+--------------------------+-----------------------------+
| $\text{Used knife}$   |  $0$                          | $0$                      | $0$                         |
+-----------------------+-------------------------------+--------------------------+-----------------------------+
| $\text{Used pistol}$  |  $0$                          | $0$                      | $0$                         |
+-----------------------+-------------------------------+--------------------------+-----------------------------+
| $\text{Sum (Error!)}$ |  $\frac{1}{12}$               | $\frac{1}{12}$           | $\frac{2}{12}$              |
+-----------------------+-------------------------------+--------------------------+-----------------------------+


## Renormalizing yields posterior beliefs
+--------------------------+----------------------------+--------------------------+-----------------------------+
|                          |$\text{He killed himself}$  |  $\text{She killed him}$ | $\text{Butler killed him}$  |
+--------------------------+----------------------------+--------------------------+-----------------------------+
| $\text{Used axe}$        |  $\frac{1}{4}$             | $\frac{1}{4}$            | $\frac{2}{4}$               |
+--------------------------+----------------------------+--------------------------+-----------------------------+
| $\text{Sum (Posterior)}$ |  $\frac{1}{4}$             | $\frac{1}{4}$            | $\frac{2}{4}$               |
+--------------------------+-----------------------------+-------------------------+-----------------------------+


## A murder mystery: Formally
- We need to renormalize by dividing by $P(\text{Axe})$

$$
P(\text{Suicide}|\text{Axe}) = \frac{P(\text{Axe}, \text{Suicide})}{P(\text{Axe})} \\
$$

- This is Bayes' rule!
- In addition to the **Sum rule**

$$
\begin{split}
P(\text{Axe}) &= P(\text{Axe}, \text{Suicide}) +  P(\text{Axe}, \text{She killed}) + P(\text{Axe}, \text{Butler killed}) \\[1ex]
              &= \sum_{H}^{3} P(\text{Axe}, H)
\end{split}
$$

- We find the **Product rule**

$$
P(\text{Suicide}|\text{Axe})P(\text{Axe}) = P(\text{Axe}, \text{Suicide})
$$


## Why most published research is false
+--------------------------+---------------+------------------+
|                          |$\mathcal{H}_0$|  $\mathcal{H}_1$ |
+--------------------------+---------------+------------------+
| reject $\mathcal{H}_0$   | $\alpha$      | $1 - \beta$      |
+--------------------------+---------------+------------------+
| keep $\mathcal{H}_0$     | 1 - $\alpha$  | $\beta$          |
+--------------------------+---------------+------------------+

## Why most published research is false
- What is the probability that my hypothesis is true, given that I have observed $p < \alpha$?

<span style = "color:white"></span>

$$
\begin{split}
P(H_1|p < \alpha) &= \frac{P(p < \alpha|H_1)P(H_1)}{P(p < \alpha)}
&= \frac{P(p < \alpha|H_1)P(H_1)}{P(p < \alpha|H_1)P(H_1) + P(p < \alpha|H_0)P(H_0)}
\end{split}
$$

<span style = "color:white"></span>

<div style="margin-top: 100px;">
- Plugging in $P(p < \alpha|H_1) = 1 - \beta$ and $P(p < \alpha|H_0) = \alpha$

<span style = "color:white"></span>

$$
P(H_1|p < \alpha) = \frac{(1 - \beta) P(H_1)}{(1 - \beta) P(H_1) + \alpha (1 - P(H_1))}
$$
</div>

## Why most published is false
```{r, echo = FALSE}
library('latex2exp')

get_ppv <- function(power, prior, alpha) (power * prior) / (power * prior + alpha * (1 - prior))

create_graph <- function(power, prior_h1, alpha = .5) {
  n_power <- length(power)
  n_ph1 <- length(prior_h1)
  
  g <- expand.grid(power, prior_h1)
  v <- Vectorize(get_ppv)
  
  d <- data.frame(
    prior_h1 = g[, 2],
    power = g[, 1],
    posterior_h1 = v(g[, 1], g[, 2], alpha)
  )
  
  ggplot(d, aes(x = power, y = posterior_h1, linetype = factor(prior_h1))) +
    geom_line() + 
    xlim(c(0, 1)) +
    ylim(c(0, 1)) +
    ylab(TeX('$$P(H_1|p < a)$$')) +
    xlab('Power') +
    geom_vline(xintercept = .21, color = 'red', linetype = 'dashed') +
    geom_vline(xintercept = .8, color = 'green', linetype = 'dashed') +
    annotate('text', .213, 0, label = 'actual power') +
    annotate('text', .794, 0, label = 'desired power') +
    scale_linetype_discrete(name = TeX('$$P(H_1)$$')) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10))
}

power <- seq(.01, .99, .01)
prior_h1 <- seq(.1, .9, .2)
create_graph(power, prior_h1, .05)
```

## Why most published research is false {.flexbox .vcenter .emphasize}

<span style = "font-size: 3em; margin-top: -500px;">
[Demo](http://shinyapps.org/apps/PPV/)
</span>


## Sequential analysis
- I might have testicular cancer, so I'll go to the doctor to do a test

$$
\begin{split}
H_0 &: \text{no cancer} \\
H_1 &: \text{cancer} \\
\end{split}
$$

- assuming the following (data for the base rate from [here](https://seer.cancer.gov/statfacts/html/testis.html))

$$
\begin{split}
P(H_1) &= .0016 \\
P(\text{pos.}|H_0) &= .1 \\
P(\text{pos.}|H_1) &= .999
\end{split}
$$
- What is the probability that I have cancer, given that I test positively?

## Sequential analysis
```{r, echo = FALSE, eval = FALSE}
get_ppv(.999, .0016, .1)
```

- We can plug it into Bayes' rule

$$
\begin{split}
P(H_1|\text{pos.}) &= \frac{P(\text{pos.}|H_1)P(H_1)}{P(\text{pos.}|H_1)P(H_1) + P(\text{pos.}|H_0)P(H_0)} \\[1ex]
&= \frac{.999 \cdot .0016}{.999 \cdot .0016 + .001 \cdot (1 - .0016)} \\[1ex]
&\approx .01575
\end{split}
$$

- Only about 1.5%
- Doing another test, I test positively again. Oh man!
- What is the probability that I have cancer now?

## Sequential analysis
- I can --- and should! --- use the posterior of the first test as the prior for the second

$$
P(H_1|\text{pos.})_{\text{Test 1}} \Rightarrow P(H_1)_{\text{Test 2}}
$$
- We arrive at
```{r, echo = FALSE, eval = FALSE}
get_ppv(.999, get_ppv(.999, .0016, .1), .1)
```

$$
\begin{split}
P(H_1|\text{pos., pos.}) &= \frac{.999 \cdot .01575}{.999 \cdot .01575 + .001 \cdot (1 - .01575)} \\[1ex]
&\approx .1379
\end{split}
$$
- The probability that I have testicular cancer still is only about 13.8%


## Probability theory
- It is a branch of mathematics based on set theory, and as such uncontroversial
- However, there are different interpretations of probability
    - *Frequentist view:* Probability is long-running frequency
    - *Bayesian view:* Probability as degree of belief
    
<span style = "color: white"></span>

- What is the probability that Angela Merkel wins the German election?
- What is the probability of a global temperature rise of 2°C in the 30 years?


## Frequentist interpretation

<span style = "font-size: 1.5em; line-height: 120%; margin-top: -500px;">
"The rational concept of probability, which is the only basis of probability calculus, applies only to problems in which either the same event repeats itself again and again ... [In] order to apply the theory of probability we must have a practically unlimited sequence of observations."

- Richard von Mises (quoted in Jackman, 2009, p. 5)
</span>

## Bayesian interpretation

<span style = "font-size: 1.5em; line-height: 120%; margin-top: -500px;">
The abandonment of superstitious beliefs about...Fairies and Witches was an
essential step along the road to scientific thinking. Probability, too, if regarded
as something endowed with some kind of objective existence, is not less a
misleading misconception, an illusory attempt to exteriorize or materialize
our true probabilistic beliefs. In investigating the reasonableness of our own
modes of thought and behaviour under uncertainty, all we require, and all that
we are reasonably entitled to, is consistency among these beliefs, and their
reasonable relation to any kind of relevant objective data (‘relevant’ in as
much as subjectively deemed to be so). This is Probability Theory (de Finetti
1974, 1975, x).

- Bruno de Finetti (quoted in Jackman, 2009, p. 7)
</span>

# Statistical models
## Statistical models
- Say I have a set of $n = 100$ observations $d = \{x_1, x_2, x_3, \ldots, x_n\}$
- You ask me about the data; what can I tell you?
```{r, echo = FALSE}
set.seed(1774)
x <- rnorm(100, 100, 10)
```

```{r}
x
```

## Statistical models
- Introduce a statistical model to reduce complexity
- A statistical model describes the relationship of one or more **latent parameters** to the observations

$$
f(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \big (-\frac{1}{2\sigma} (x - \mu)^2 \big)
$$

```{r, echo = FALSE}
mu <- mean(x)
sigma <- sd(x)

ggplot(data.frame(x = x), aes(x = x)) +
  geom_histogram(aes(y = ..density..), bins = 20, fill = 'grey46', col = 'black') +
  stat_function(fun = function(y) dnorm(y, mean = mu, sd = sigma), col = 'firebrick', size = 1.5)
```

## Statistical models
- I used a normal distribution for the data
- Now I can describe the data with just two numbers --- the mean $\mu$ and the standard deviation $\sigma$

```{r}
c(mean(x), sd(x))
```

- But there are other possible statistical models I could use!


## Statistical models
- Statistical models need not be appropriate; in fact, they can be horribly misspecified!

$$
f(x; a, b) = \frac{1}{b - a}
$$
```{r, echo = FALSE}
ggplot(data.frame(x = x), aes(x = x)) +
  geom_histogram(aes(y = ..density..), bins = 20, fill = 'grey46', col = 'black') +
  stat_function(fun = function(y) dunif(y, min = 50, max = 150), col = 'firebrick', size = 1.5)
```

## Statistical models: Notation
- Statistical models can be specified as **directed acyclic graphs**

<span style = "color: white"></span>

<center>
<img src="images/uniform-normal-models.png" />
</center>


## Bread and butter
- A slice of bread with butter falls on the floor
- It can either land with butter up (**0**) or butter down (**1**)

<span style = "color: white"></span>

- Questions
    - <span style = "color:red">What is the propensity of the bread to fall on the floor with the butter side down?</span>
    - <span style = "color:red">How do I describe the data-generating process? Any suggestions?</span>


## Bread and butter
- First, note that the outcome is binary. The random variable describing the outcome is $X = \{0, 1\}$
- A good, tried suggestion would be the following statistical model

$$
\begin{align*}
f(x; \theta) &= \theta^x (1 - \theta)^{1 - x} \\[2ex]
             &= \begin{cases} \theta & \text{if } x = 1 \\ 1 - \theta & \text{otherwise} \end{cases}
\end{align*}
$$
- where $\theta$ is the latent parameter whose value influences the outcome


## Statistical models: Bread and butter
- How do I describe $n$ events?
- I make a <span style = "color:red">very contentious assumption</span>: **independent and identically distributed samples**
- Assume I have $n = 20$ data points $d = \{x_1, x_2, x_3, \ldots, x_n\}$

## Statistical models: Bread and butter

$$
\begin{align*}
f(x; \theta) &\stackrel{\text{i.i.d.}}{=} f(x_1; \theta) \cdot f(x_2; \theta) \cdot \ldots \cdot f(x_n; \theta) \\
             &= \prod^n_{i=1} f(x_i; \theta) \\
             &= \prod^n_{i=1} \theta^{x_i} (1 - \theta)^{1 - x_i} \\
             &= \theta^{\sum^n_{i = 1} x_i} (1 - \theta)^{\sum^n_{i = 1} 1 - x_i}
\end{align*}
$$

- We don't care about the outcome order; e.g., whether $x = \{1, 0 \}$ or $x = \{0, 1\}$


## Binomial likelihood
- Neglecting the order, we only model the number of falls with butter on the floor
- Introducing a new random variable $Y$

$$
Y = \sum_{i=1}^n x_i
$$

- and noting that there are

$$
n\cdot(n - 1)\cdot \ldots \cdot (n - y + 1) = \frac{n!}{y!(n - y)!} = {n \choose y}
$$

- possible ways of obtaining $Y = y$ successes yields the **Binomial likelihood**


## Binomial likelihood

$$
\begin{align*}
f(x; \theta) &\stackrel{\text{i.i.d.}}{=} f(x_1; \theta) \cdot f(x_2; \theta) \cdot \ldots \cdot f(x_n; \theta) \\
             &= \prod^n_{i=1} f(x_i; \theta) \\
             &= \prod^n_{i=1} \theta^{x_i} (1 - \theta)^{1 - x_i} \\
             &= \theta^{\sum^n_{i = 1} x_i} (1 - \theta)^{\sum^n_{i = 1} 1 - x_i} \\[4ex]
f(y; \theta, n) &= {n \choose y} \theta^{\sum^n_{i = 1} x_i} (1 - \theta)^{\sum^n_{i = 1} 1 - x_i} \hspace{3em} \text{Neglecting order} \\[1ex]
                &= {n \choose y} \theta^{y} (1 - \theta)^{n - y} \hspace{7em} \text{Introducing  } Y = \sum^n_{i=1} x_i \\
\end{align*}
$$

## Binomial likelihood
![](images/binomial-model.png)


## Binomial likelihood
- $\theta$ describes how the data will turn out
- In other words: the likelihood is a distribution for the data

```{r, echo = FALSE}
n <- 20
xx <- seq(n)
d <- data.frame(
  x = rep(xx, 3),
  theta = factor(rep(c(.2, .5, .8), each = n)),
  y = c(dbinom(xx, n, prob = .2), dbinom(xx, n, prob = .5), dbinom(xx, n, prob = .8))
)

ggplot(d, aes(x = x, y = y, group = theta)) +
  geom_bar(stat = 'identity', aes(fill = theta)) +
  facet_wrap(~ theta) +
  xlab('y') +
  ylab('Density') +
  scale_fill_discrete(name = expression(theta))
```

## Binomial likelihood
```{r, echo = FALSE, height = 3, width = 3}
library('shiny')

mathjax_URL <- 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full'

shinyApp(
  options = list(width = "25%", height = "25%"),
  ui = shinyUI(fluidPage(
    sidebarLayout(
      sidebarPanel(
        tags$head(
          tags$script(src = mathjax_URL, type = 'text/javascript'),
          tags$script("MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});", type = 'text/x-mathjax-config')
    ),
        sliderInput("p", label = HTML('$$\\theta$$'),
                    min = 0, max = 1, value = 0.5, step = 0.01),
        sliderInput("n", label = "n",
                    min = 1, max = 100, value = 20, step = 1)
        ),
      mainPanel(
        plotOutput("LRplot", height="400px")
        )
      )
   )),
   server = function(input, output) {
      plot_dat <- function(n, p) {
        x <- seq(n)
        y <- dbinom(x, n, p)
        d <- data.frame(
          x = x,
          theta = p,
          y = y / max(y)
        )
        
        ggplot(d, aes(x = x, y = y)) +
          geom_bar(stat = 'identity') +
          xlab('y') +
          ylab('Density') +
          ggtitle(TeX(paste0('Data distribution for ', '$\\theta$ = ', round(p, 2)))) +
          theme(plot.title = element_text(hjust = .5))
      }
      
      output$LRplot <- renderPlot({
        plot_dat(input$n, input$p)
      })
    }
)
```

# Fundamental theorems

## Two corners of (frequentist) statistics
- Law of the large number
$$
\begin{split}
\overline{X}_n &= \frac{1}{n}(X_1 + \ldots + X_n) \\[2ex]
\overline{X}_n &\rightarrow \mu \qquad \text{for} \qquad n \rightarrow \infty
\end{split}
$$
- Computing the mean of a random variable $X$

$$
\begin{split}
\mathbb{E}[X] &= \int x \cdot P(x) \mathrm{d}x \\
              &= \lim_{N \to \infty} \frac{1}{N} \sum_{x_i \sim P}^{N} x_i
\end{split}
$$

## Law of large numbers
- Computing the mean of rolling a six-sided die
- The accuracy of the empirical statistic (the mean) increases with sample size

```{r, echo = FALSE}
set.seed(174)
N <- seq(2, 1000, 10)
roll_dice <- function(n) mean(sample(1:6, n, replace = TRUE))

d <- data.frame(
  N = N,
  estimate = sapply(Vectorize(roll_dice)(N), mean)
)

ggplot(d, aes(x = N, y = estimate)) +
  geom_line() +
  ylab('Sample mean') +
  xlab('Sample size') +
  ylim(c(0, 6)) +
  # stat_function(fun = function(N) (1/sqrt(N)) + 3.5, colour = 'blue') +
  # stat_function(fun = function(N) (-1/sqrt(N)) + 3.5, colour = 'blue') +
  geom_hline(yintercept = 3.5, colour = 'red', linetype = 'dashed')
```


##
```{r, echo = FALSE, fig.width = 9, fig.height = 6.5}
library('dplyr')
library('tidyr')

ng <- 50
nt <- 10000

d <- data.frame(
  Beta = replicate(nt, mean(rbeta(ng, 1, 1))),
  Poisson = replicate(nt, mean(rpois(ng, lambda = 10))),
  Exponential = replicate(nt, mean(rexp(ng))),
  Gamma = replicate(nt, mean(rgamma(ng, 1, 1))),
  Weibull = replicate(nt, mean(rweibull(ng, 1, 1))),
  Chisq = replicate(nt, mean(rchisq(ng, 1))),
  Gaussian = replicate(nt, mean(rnorm(ng))),
  Student.t = replicate(nt, mean(rt(ng, 20))),
  Cauchy = replicate(nt, mean(rcauchy(ng, 1)))
)

dg <- gather(d, dist, value) %>%
  mutate(dist = factor(dist, levels = c('Beta', 'Poisson', 'Exponential',
                                        'Gamma', 'Weibull', 'Chisq',
                                        'Gaussian', 'Student.t', 'Cauchy')))

ggplot(dg, aes(x = value)) +
  geom_histogram() +
  xlab('') +
  ylab('') +
  ggtitle('Sampling distribution of the Mean') +
  theme(plot.title = element_text(hjust = .5)) +
  facet_wrap(~ dist, scales = 'free')
```
    
# Statistical models cont'd

## Bread and butter
- I want to learn $\theta$ --- what are good values for it, given the data below?

```{r, echo = FALSE, fig.width = 4, figh.height = 2}
set.seed(1774)

x <- rbinom(20, 1, prob = .7)
y <- sum(x)
n <- length(x)

ggplot(data.frame(x = x), aes(x = x)) +
  geom_bar()
```


## Bread and butter
- A natural question to ask is: **How likely is the data given certain parameter values?**
- Let's compare $\theta = .5$ and $\theta = .75$ on our flipping data ($y = 15, n = 20$)

$$
\begin{align*}
f(Y = 15|n = 20, \theta = .5)  &= {20 \choose 15} .50^{15} (1 - .50)^{20 - 15} \approx 0.0147 \\[1ex]
f(Y = 15|n = 20, \theta = .75) &= {20 \choose 15} .75^{15} (1 - .75)^{20 - 15} \approx 0.2033
\end{align*}
$$


## Bread and butter
- Let's plot the likelihood of the data as a function of $\theta$

```{r, echo = FALSE}
p1 <- .75
p2 <- .5
llh.p1 <- dbinom(y, n, p1)
llh.p2 <- dbinom(y, n, p2)

ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
  stat_function(fun = function(theta) dbinom(y, n, prob = theta)) +
  geom_segment(x = p1, y = -1, xend = p1, yend = llh.p1, aes(col = 'blue'), linetype = 'dashed') +
  geom_segment(x = p2, y = -1, xend = p2, yend = llh.p2, aes(col = 'red'), linetype = 'dashed') +
  geom_point(x = p1, y = llh.p1, size = 2) +
  geom_point(x = p2, y = llh.p2, size = 2) +
  scale_colour_manual(name = '', values = c('red', 'blue'),
                      labels = c(expression(hat(theta)[1]), expression(hat(theta)[2]))) +
  annotate('text', x = .15, y = .18, label = 'y = 15\nn = 20', size = 6) +
  ylab('Likelihood') +
  xlab(expression(theta))
```

## Likelihood ratios
```{r, echo = FALSE, height = 3, width = 3}
shinyApp(
  options = list(width = "25%", height = "25%"),
  ui = shinyUI(fluidPage(
    sidebarLayout(
      sidebarPanel(
        tags$head(
          tags$script(src = mathjax_URL, type = 'text/javascript'),
          tags$script("MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});", type = 'text/x-mathjax-config')
    ),
        sliderInput("p1", label = HTML('$$\\theta_1$$'),
                    min = 0, max = 1, value = 0.75, step = 0.01),
        sliderInput("p2", label = HTML('$$\\theta_2$$'),
                    min = 0, max = 1, value = 0.5, step = 0.01),
        sliderInput("y", label = "y",
                    min = 1, max = 50, value = 15, step = 1),
        sliderInput("n", label = "n",
                    min = 1, max = 50, value = 20, step = 1)
        ),
      mainPanel(
        plotOutput("LRplot", height="400px")
        )
      )
   )),
   server = function(input, output) {
      plot.LR <- function(y, n, p1, p2) {
        # # adapted from blog mentioned above
        # MLE <- dbinom(k, N, k/N)
        # L1 <- dbinom(k, N, prob = p1) / MLE
        # L2 <- dbinom(k, N, prob = p2) / MLE
        # 
        # curve((dbinom(k, N, x) / max(dbinom(k, N, x))), xlim = c(0, 1),
        #       ylab = "Likelihood", xlab = "Probability of correct answer",
        #       las = 1, main = "Likelihood function for binomials", lwd = 3,
        #       cex.axis = 1.5, cex.lab = 1.5, cex.main = 1.5)
        # 
        # points(p1, L1, cex = 2, pch = 21, bg = "cyan")
        # points(p2, L2, cex = 2, pch = 21, bg = "cyan")
        # lines(c(p1, p2), c(L1, L1), lwd = 3, lty = 2, col = "cyan")
        # lines(c(p2, p2), c(L1, L2), lwd = 3, lty = 2, col = "cyan")
        # abline(v = k/N, lty = 5, lwd = 1, col = "grey73")
        llh.p1 <- dbinom(y, n, p1)
        llh.p2 <- dbinom(y, n, p2)
        
        ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
          stat_function(fun = function(theta) dbinom(y, n, prob = theta)) +
          geom_segment(x = p1, y = -1, xend = p1, yend = llh.p1, aes(col = 'blue'), linetype = 'dashed') +
          geom_segment(x = p2, y = -1, xend = p2, yend = llh.p2, aes(col = 'red'), linetype = 'dashed') +
          geom_point(x = p1, y = llh.p1, size = 2) +
          geom_point(x = p2, y = llh.p2, size = 2) +
          scale_colour_manual(name = '', values = c('red', 'blue'),
                              labels = c(expression(hat(theta)[1]), expression(hat(theta)[2]))) +
          ylab('Likelihood') +
          xlab(expression(theta)) +
          ggtitle(TeX(paste0('Likelihood ratio of ',
                             '$\\frac{f(y; n, \\theta_1)}{f(y; n, \\theta_2)}$ = ',
                             get.LR(y, n, p1, p2)))) +
          theme(plot.title = element_text(hjust = .5))
      }
      
      get.LR <- function(y, n, p1, p2) {
        MLE <- dbinom(y, n, k/N)
        L1 <- dbinom(y, n, prob = p1) / MLE
        L2 <- dbinom(y, n, prob = p2) / MLE
        round(L1 / L2, 2)
      }
      
      output$LRplot <- renderPlot({
        plot.LR(input$y, input$n, input$p1, input$p2)
      })
    }
)
```


# Classical and Bayesian statistics
## A neutral Bird's eye view
  Ronald Fisher                     Neyman-Pearson                       Bayesian
----------------------              ---------------------------          ---------------------------
Probability as long-run frequency   Probability as long-run frequency    Probability as degree of belief
*p*-value                           *p*-value                            Bayes factors
$\alpha$                            $\alpha, \beta$                      -
Parameters are fixed                Parameters are fixed                 Parameters are random
Pre-data                            Pre-data                             Post-data
Epistemic approach                  Behavioural approach                 Epistemic approach
Various estimators                  Various estimators                   Bayes' rule
-                                   Confidence Intervals                 Credible intervals
Point estimates                     Point estimates                      Posterior distributions


## A polemic Bird's eye view
 Classical Statistics |     Bayesian Statistics
---------------------:|:---------------------------
Ad-hoc                |    Axiomatic
Incoherent            |    Coherent
Paradoxical           |    Intuitive
Irrational            |    Rational
Ugly                  |    Pretty
Irrelevant            |    Relevant
what's taught         |    what's not taught

<div id="contrast">borrowed from [EJ Wagenmakers](https://docs.google.com/file/d/0B-Ww24m3ZkEyMEpudlVsX3pRVzA/edit)</div>

## Practical example
- **Data example**
    - We observe $n = 20$ pieces of bread with butter falling down
    - $y = 15$ land on the floor with the butter down
    - Questions
        - <span style = 'color: red'>Estimation</span>: What are likely values for $\theta$?
        - <span style = 'color: red'>Testing</span>: Is bread with butter more likely to fall butter down?
        - <span style = 'color: red'>Prediction</span>: How likely is the next piece to fall butter down?
        
Problem           Classical statistics     Bayesian statistics
-----------       --------------------     --------------------
Estimation        Maximum Likelihood, CIs  Posterior distribution (**Bayes' rule**)
Testing           *p*-value                Bayes factor (**Bayes' rule**)
Prediction        Plug-in Estimate         Account for uncertainty using **Bayes' rule**


# Classical statistics

## Bread and Butter: Estimation
- Many different estimators, but most popluar is maximum likelihood estimate; $\hat{\theta} = \frac{y}{n}$

```{r, echo = FALSE}
y <- 15
n <- 20

ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
  stat_function(fun = function(theta) dbinom(y, n, prob = theta)) +
  geom_segment(x = y/n, xend = y/n, y = -1, yend = dbinom(y, n, prob = y/n), color = 'red', linetype = 'dashed') +
  geom_point(x = y/n, y = dbinom(y, n, prob = y/n), size = 2) +
  ylab('Likelihood') +
  xlab(expression(theta))
```


## Bread and Butter: Estimation
- The frequentist interpretation of probability requires the assumption of repeated sampling

<span style = "color: white"></span>

<span style = "color: red">
"We assume that we sample repeatedly from a population; each time we compute an estimate based on the sample. The distribution of these estimates --- the sampling distribution --- gives the precision of the specific estimate."
</span>

<span style = "color: white"></span>

- In other words

<span style = "color: white"></span>

<span style = "color: red">
"We conduct an experiment. We then assume that we conduct exactly the same experiment over and over again. For
each such experiment, we compute a statistic (say the mean). Plotting these statistics yields the sampling distribution (of the mean)."
</span>

## Bread and Butter: Estimation
```{r}
conduct_experiment <- function(y, n) rbinom(n, 1, prob = y / n)
replicate(10, conduct_experiment(y, n))
```

## Bread and Butter: Estimation
```{r}
sampling_distribution <- apply(replicate(10000, conduct_experiment(y, n)), 2, mean)
```

```{r, echo = FALSE, fig.width = 7, fig.height = 5}
title <- TeX(paste0('Sampling distribution of ', '$\\theta$'))

p <- y / n
pse <- sqrt((p * (1 - p)) / n)
s <- data.frame(theta = sampling_distribution)

ggplot(s, aes(x = theta)) +
  geom_histogram(aes(y = ..density..), bins = 21, color = 'grey70') +
  xlim(c(0, 1)) +
  ggtitle(title) +
  xlab(expression(theta)) +
  theme(plot.title = element_text(hjust = .5)) +
  stat_function(fun = function(theta) dnorm(theta, p, pse), color = 'red')
```

## Confidence intervals
```{r}
binom::binom.confint(y, n)
```

## Confidence intervals
```{r, echo = FALSE}
simulate_cis <- function(x, n, times = 100, n_boot = 1000) {
    p <- sum(x) / n
    cis <- matrix(NA, nrow = times, ncol = 2)
    
    for (i in seq(times)) {
      dat <- sample(c(0, 1), size = n, prob = c(1-p, p), replace = TRUE)
      
      p.hat <- mean(dat)
      se <- sd(dat) / sqrt(n)
      
      cis[i, 1] <- p.hat - 1.96 * se
      cis[i, 2] <- p.hat + 1.96 * se
    }
    
    within <- apply(cis, 1, function(row) row[1] < p && row[2] > p)
    cbind(cis, within, p)
}

plot_cis <- function(cis, times) {
  p <- cis[1, 4]
  x <- seq(0, times)
  z <- seq(0, 1, 1/times)
  
  plot(x, z, type = 'n', xlab = 'i-th Experiment',
       ylab = TeX('$\\theta$'), main = TeX(paste0('95% CIs for ', '$\\theta$')))
  
  abline(p, 0, lwd = 3)
  
  arrows(x0 = x, y0 = cis[, 1], x1 = x, length = 0, lwd = 1.2,
         y1 = cis[, 2], col = ifelse(cis[, 3], 'black', 'red'))
}

times <- 100
cis <- simulate_cis(y, n, times = times)
```

```{r, echo = FALSE}
plot_cis(cis, times)
```

## Confidence intervals {.flexbox .vcenter .emphasize}

<span style = "font-size: 3em; margin-top: -500px;">
[Demo](http://rpsychologist.com/d3/CI/)
</span>


## $p$-values
<center>
<img src="images/pvalue.png" />
</center>

## $p$-values
- We replicate data under the assumption that $H_0$ is true, i.e, $\theta = .5$
```{r}
conduct_experiment_h0 <- function(n) sum(rbinom(n, 1, prob = .5))
yrep <- replicate(5000, conduct_experiment_h0(n))

head(yrep, 10) # peak at some experiment outcomes
```

- Then we compute the probability that these data are at least as extreme as the one we observed
```{r}
mean(yrep >= y) * 2
```

## $p$-values
```{r}
binom.test(y, n)
```

## Some issues with $p$-values
- Are uniformly distributed under $H_0$
    - By design, in order to have $\alpha$ number of rejections under $H_0$
    - But therefore does not allow stating evidence in favour of $H_0$
    
- Is not really a means to compare models, but to check models
    - Conditional on the model (e.g., $H_0$), are the data surprising?
    
- Is rather obscure and frequently misinterpreted
    - Which can have serious consequences (e.g., Sally Clark, medicine)
    
- Inference using $p$-values can't distinguish between being underpowered or $H_1$ being true

    
# Bayesian statistics
## Bayesian estimation
- Recap of the murder mystery
- **Joint probability**

$$
P(\text{Axe}, \text{Suicide}) = \frac{1}{12}
$$
- **Marginal probability**

$$
\begin{split}
P(\text{Suicide}) &= \frac{1}{3} \\[1ex]
                            &= P(\text{Axe}, \text{Suicide}) +  P(\text{Knife}, \text{Suicide}) + P(\text{Pistol}, \text{Suicide}) \\[1ex]
                            &= \frac{1}{12} + \frac{1}{12} + \frac{2}{12} \\[2ex]
                            &\Rightarrow \sum_{W}^{3} P(W, \text{Suicide}) \qquad \text{Sum rule}
\end{split}
$$



## Bayesian estimation
+-----------------------+-------------------------------+--------------------------+-----------------------------+
|                       |$\text{He killed himself}$     |  $\text{She killed him}$ | $\text{Butler killed him}$  |
+-----------------------+-------------------------------+--------------------------+-----------------------------+
| $\text{Used axe}$     |  $\frac{1}{12}$               | $\frac{1}{12}$           | $\frac{2}{12}$              |
+-----------------------+-------------------------------+--------------------------+-----------------------------+
| $\text{Used knife}$   |  $\frac{1}{12}$               | $\frac{3}{12}$           | $\frac{1}{12}$              |
+-----------------------+-------------------------------+--------------------------+-----------------------------+
| $\text{Used pistol}$  |  $\frac{2}{12}$               | $0$                      | $\frac{1}{12}$              |
+-----------------------+-------------------------------+--------------------------+-----------------------------+
| $\text{Sum (Prior)}$  |  $\frac{1}{3}$                | $\frac{1}{3}$            | $\frac{1}{3}$               |
+-----------------------+-------------------------------+--------------------------+-----------------------------+


## Bayesian estimation
- Recap of why most published research is wrong
- What is the probability that my hypothesis is true, given that I have observed $p < \alpha$?

$$
\begin{split}
P(H_1|p < \alpha) &= \frac{P(p < \alpha|H_1)P(H_1)}{P(p < \alpha)}
&= \frac{P(p < \alpha|H_1)P(H_1)}{P(p < \alpha|H_1)P(H_1) + P(p < \alpha|H_0)P(H_0)}
\end{split}
$$

<span style = "color:white"></span>

- Recap of testicular cancer
$$
\begin{split}
P(H_1|\text{pos.}) &= \frac{P(\text{pos.}|H_1)P(H_1)}{P(\text{pos.}|H_1)P(H_1) + P(\text{pos.}|H_0)P(H_0)} \\[1ex]
&= \frac{.999 \cdot .0016}{.999 \cdot .0016 + .001 \cdot (1 - .0016)} \\[1ex]
&\approx .01575
\end{split}
$$

## Bayesian estimation
- Uses Bayes' rule
$$
\underbrace{p(\theta|y)}_{\text{Posterior}} = \frac{\overbrace{f(y; \theta)}^{\text{Likelihood}}\overbrace{p(\theta)}^{\text{Prior}}}{\underbrace{p(y)}_{\text{Marginal Likelihood}}}
$$

- where, applying the sum rule of probability

$$
p(y) = \int_{\theta} f(y; \theta)p(\theta) \mathrm{d}\theta
$$

## Prior: Beta distribution

$$
p(\theta|\alpha, \beta) = \frac{1}{\text{Beta}(\alpha, \beta)} \theta^{\alpha - 1}(1 - \theta)^{\beta - 1}
$$
<center>
  <img src="images/beta_overview.png" width="600px" height="400px"/>
</center>

## Prior: Beta distribution
```{r, echo = FALSE, height = 3, width = 3}
shinyApp(
  options = list(width = "25%", height = "25%"),
  ui = shinyUI(fluidPage(
    sidebarLayout(
      sidebarPanel(
        tags$head(
          tags$script(src = mathjax_URL, type = 'text/javascript'),
          tags$script("MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});", type = 'text/x-mathjax-config')
    ),
        sliderInput("a", label = 'a',
                    min = .5, max = 20, value = 1, step = .5),
        sliderInput("b", label = "b",
                    min = .5, max = 20, value = 1, step = .5)
        ),
      mainPanel(
        plotOutput("LRplot", height="400px")
        )
      )
   )),
   server = function(input, output) {
      plot_dat <- function(a, b) {
        ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
          stat_function(fun = function(x) dbeta(x, a, b)) +
          xlab(expression(theta)) +
          ylab('Density') +
          ggtitle(paste0('Beta(', a, ', ', b, ')', ' with mean ', round(a / (a + b), 2))) +
          theme(plot.title = element_text(hjust = .5))
      }
      
      output$LRplot <- renderPlot({
        plot_dat(input$a, input$b)
      })
    }
)
```


## Bayesian graphical model
<center>
  <img src="images/binomial-model-bayes.png" />
</center>


## Bayesian estimation
- Need to multiply the likelihood with the prior, and renormalize
$$
p(\theta|y) = \frac{f(y; \theta)p(\theta)}{\int_{\theta}f(y; \theta)p(\theta)\mathrm{d}\theta} \\[1ex]
$$
- which, for our example, yields
$$
\begin{split}
p(\theta|y) &= \frac{{n \choose y} \theta^y (1 - \theta)^{n - y} \frac{1}{\mathcal{B}(a, b)} \theta^{\alpha - 1}(1 - \theta)^{\beta - 1}}{\int_{\theta}{n \choose y} \theta^y (1 - \theta)^{n - y} \frac{1}{\mathcal{B}(a, b)} \theta^{\alpha - 1}(1 - \theta)^{\beta - 1}\mathrm{d}\theta}
\end{split}
$$
- with some rearranging, we find that

$$
p(\theta|y) = \frac{1}{\text{Beta}(y + \alpha, n - y + \beta)} \theta^{y + \alpha - 1}(1 - \theta)^{n - y + \beta - 1}
$$


## Bayesian updating
```{r, echo = FALSE, height = 3, width = 3}
library('shiny')
# the code (with a little cleaning up) for the visualisations is from
# http://alexanderetz.com/2015/07/25/understanding-bayes-updating-priors-via-the-likelihood/
# Alex Etz runs a really nice blog -- go check it out!

shinyApp(
  options = list(width = "25%", height = "25%"),
  ui = shinyUI(fluidPage(
    sidebarLayout(
      sidebarPanel(
        sliderInput("a", label = "a", min = 1, max = 50, value = 1, step = 1),
        sliderInput("b", label = "b", min = 1, max = 50, value = 1, step = 1),
        sliderInput("k", label = "y", min = 1, max = 50, value = 15, step = 1),
        sliderInput("N", label = "n", min = 1, max = 50, value = 20, step = 1),
        htmlOutput("BF_10")
        ),
      mainPanel(
        plotOutput('update_plot', height='400px')
        )
      )
    )
  ),
  server = function(input, output) {
        update_plot <- function(a = 1, b = 1, k = 0, N = 0, null = NULL, CI = NULL, ymax = 'auto') {
          x <- seq(.001, .999, .001) ## set up for creating the distributions
          y1 <- dbeta(x, a, b) # data for prior curve
          y3 <- dbeta(x, a + k, b + N - k) # data for posterior curve
          y2 <- dbeta(x, 1 + k, 1 + N - k) # data for likelihood curve, plotted as the posterior from a beta(1,1)
          y.max <- ifelse(is.numeric(ymax), ymax, 1.25 * max(y1, y2, y3, 1.6))
          title <- paste0('Beta(', a, ', ', b, ')', ' to Beta(', a + k, ', ', b + N - k, ')')
          
          plot(x, y1, xlim = c(0, 1), ylim = c(0, y.max), type = 'l', ylab = 'Density', lty = 2,
               xlab = TeX('$\\theta$'), las = 1, main = title, lwd=3,
               cex.lab = 1.5, cex.main = 1.5, col = 'skyblue', axes = FALSE)
          
          axis(1, at = seq(0, 1, .2)) #adds custom x axis
          axis(2, las = 1) # custom y axis
          
          if (N != 0) {
              # if there is new data, plot likelihood and posterior
              lines(x, y2, type = 'l', col = 'darkorange', lwd = 2, lty = 3)
              lines(x, y3, type = 'l', col = 'darkorchid1', lwd = 5)
              legend('topleft', c('Prior', 'Posterior', 'Likelihood'),
                     col = c('skyblue', 'darkorchid1', 'darkorange'), 
                     lty = c(2, 1, 3), lwd = c(3, 5, 2), bty = 'n',
                     y.intersp = 1, x.intersp = .4, seg.len =.7)
                  
              ## adds null points on prior and posterior curve if null is specified and there is new data
              if (is.numeric(null)) {
                      ## Adds points on the distributions at the null value if there is one and if there is new data
                      points(null, dbeta(null, a, b), pch = 21, bg = 'blue', cex = 1.5)
                      points(null, dbeta(null, a + k, b + N - k), pch = 21, bg = 'darkorchid', cex = 1.5)
                      abline(v=null, lty = 5, lwd = 1, col = 'grey73')
                      ##lines(c(null,null),c(0,1.11*max(y1,y3,1.6))) other option for null line
                }
          }
          
          ## Specified CI% but no null? Calc and report only CI
          if (is.numeric(CI) && !is.numeric(null)) {
                CI.low <- qbeta((1 - CI)/2, a + k, b + N - k)
                CI.high <- qbeta(1 - (1 - CI)/2, a + k, b + N - k)
                
                SEQlow <- seq(0, CI.low, .001)
                SEQhigh <- seq(CI.high, 1, .001)
                
                ## Adds shaded area for x% Posterior CIs
                cord.x <- c(0, SEQlow, CI.low) ## set up for shading
                cord.y <- c(0, dbeta(SEQlow, a + k, b + N - k), 0) ## set up for shading
                polygon(cord.x, cord.y, col='orchid', lty= 3) ## shade left tail
                cord.xx <- c(CI.high, SEQhigh, 1) 
                cord.yy <- c(0, dbeta(SEQhigh, a + k, b + N - k), 0)
                polygon(cord.xx, cord.yy, col='orchid', lty=3) ## shade right tail
                return(list('Posterior CI lower' = round(CI.low, 3),
                            'Posterior CI upper' = round(CI.high, 3)))
          }
          
          ## Specified null but not CI%? Calculate and report BF only 
          if (is.numeric(null) && !is.numeric(CI)){
              null.H0 <- dbeta(null, a, b)
              null.H1 <- dbeta(null, a + k, b + N - k)
              CI.low <- qbeta((1 - CI)/2, a + k, b + N - k)
              CI.high <- qbeta(1 - (1 - CI)/2, a + k, b + N - k)
              return(list('BF01 (in favor of H0)' = round(null.H1/null.H0, 3),
                          'BF10 (in favor of H1)' = round(null.H0/null.H1, 3)))
          }
          
          ## Specified both null and CI%? Calculate and report both
          if (is.numeric(null) && is.numeric(CI)){
                  null.H0 <- dbeta(null, a, b)
                  null.H1 <- dbeta(null, a + k, b + N - k)
                  CI.low <- qbeta((1 - CI)/2, a + k, b + N - k)
                  CI.high <- qbeta(1 - (1 - CI)/2, a + k, b + N - k)
                  
                  SEQlow <- seq(0, CI.low, .001)
                  SEQhigh <- seq(CI.high, 1, .001)
                  
                  ## Adds shaded area for x% Posterior CIs
                  cord.x <- c(0, SEQlow, CI.low) ## set up for shading
                  cord.y <- c(0, dbeta(SEQlow, a + k, b + N - k), 0) ## set up for shading
                  polygon(cord.x, cord.y, col = 'orchid', lty = 3) ## shade left tail
                  cord.xx <- c(CI.high, SEQhigh, 1) 
                  cord.yy <- c(0, dbeta(SEQhigh, a + k, b + N - k), 0)
                  polygon(cord.xx, cord.yy, col = 'orchid', lty = 3) ## shade right tail
                  return(list('BF01 (in favor of H0)' = round(null.H1/null.H0, 3),
                              'BF10 (in favor of H1)' = round(null.H0/null.H1, 3),
                              'Posterior CI lower' = round(CI.low, 4),
                              'Posterior CI upper' = round(CI.high, 3)))
          }
        }
        output$update_plot <- renderPlot({
          null <- .5
          update_plot(input$a, input$b, input$k, input$N, null = null)
        })
        
        output$BF_10 <- renderText({
          a <- input$a
          b <- input$b
          k <- input$k
          N <- input$N
          label <- 'BF<span style="font-size: .6em;">10</span>: '
          BF10 <- 1 / (dbeta(.5, a + k, b + N - k) / dbeta(.5, a, b))
          paste(label, round(BF10, 3))
        })
  }
)
```

## Bayesian model comparison
- In classical statistics, we test

$$
H_0: \theta = .5 \\
H_1: \theta \neq .5 \\
$$

- but the model which instantiates $H_1$ makes no predictions whatsoever!
- We could use likelihoods to compare the hypotheses, such as

$$
\begin{split}
H_0&: \theta = .5 \\
H_1&: \theta = .75
\end{split}
$$

- but this is too demanding; why .75? Why not .80? Or .30?


## Bayesian model comparison
- We are uncertain about alternative values for $\theta$
- Bayes solution: quantify our uncertainty with a prior distribution

$$
\begin{split}
H_0&: \theta = .5 \\
H_1&: \theta \sim \mathcal{Beta}(\alpha, \beta)
\end{split}
$$

- Now both models make predictions!
- Bayesian model comparison pits the predictive accuracy of two models against each other
$$
\begin{split}
p(y|M_0) &= \int_{\theta} p(y, \theta|M_0)\mathrm{d}\theta \\[1ex]
         &= \int_{\theta} p(y|M_0, \theta)p(\theta|M_0)\mathrm{d}\theta \\[1ex]
         &= f(y|M_0, \theta = .5) \\[2ex]
         &= {20 \choose 15} .50^{15} (1 - .50)^{20 - 15} \approx 0.0147
\end{split}
$$

## Bayesian model comparison
- For $M_1$, it's a bit more complicated

$$
\begin{split}
p(y|M_1) &= \int_{\theta} p(y, \theta|M_1)\mathrm{d}\theta \\[1ex]
         &= \int_{\theta} p(y|M_1, \theta)p(\theta|M_1)\mathrm{d}\theta \\[1ex]
         &= \int_{\theta} {20 \choose 15} \theta^{15} (1 - \theta)^{20 - 15} \frac{1}{\text{Beta}(a, b)}\theta^{\alpha - 1}(1 - \theta)^{\beta - 1}\mathrm{d}\theta \\[1ex]
         &\approx \frac{1}{N} \sum_{\theta \sim \mathcal{Beta}(\alpha, \beta)}^N {20 \choose 15} \theta^{15} (1 - \theta)^{20 - 15} \hspace{3em} \text{Monte Carlo integration} \\[1ex]
         &\approx 0.047
\end{split}
$$

## Bayesian model comparison
- How do we update our beliefs across models?
$$
\begin{split}
\underbrace{\frac{p(M_0 \mid y)}{p(M_1 \mid y)}}_{\text{posterior odds}} &=
\underbrace{\frac{p(y \mid M_0)}{p(y \mid M_1)}}_{\text{Bayes factor}} \, \cdot \underbrace{\frac{p(M_0)}{p(M_1)}}_{\text{prior odds}} \\[1ex]
\frac{p(M_0|y)}{p(M_1|y)} &= \frac{.0147}{.0477} \cdot \frac{1/2}{1/2}
\approx .31
\end{split}
$$

```{r, echo = FALSE, fig.width = 5, fig.height = 3}
post_m0 <- 1 / (1 + .31)
post_m1 <- 1 - post_m0

d <- data.frame(p = c(1/2, post_m0, 1/2, post_m1),
                m = factor(rep(c('M_0', 'M_1'), each = 2)),
                distribution = relevel(factor(rep(c('prior', 'posterior'), 2)), ref = 'prior'))

ggplot(d, aes(x = m, y = p, fill = distribution)) +
  geom_bar(stat = 'identity') +
  xlab('Model') +
  ylab('Posterior probability') +
  facet_wrap(~ distribution) +
  theme(legend.position = 'none')
```


## Bayes factor
- is the non-plus-ultra of model comparison
    - complex models can predict many data patterns
    - thus spread out their prior probability
    - this gets factored into the marginal likelihood and decreases it
    - instantiates an **automatic Ockham's razor**
\
\
- looks at the functional form of the model, compared to
    - $AIC = -2 \log p(\textbf{y}|\hat \theta) + 2\cdot k$
    - $BIC = -2 \log p(\textbf{y}|\hat \theta) + \log n \cdot k$
- which penalize just based on the number of parameters $k$

## Thermometer of evidence
<center>
<img src="images/bayes-factor-labels.png" />
</center>

## Lindley's Paradox
```{r}
k <- 49581
N <- 98451
```

<div style = "float:left; width:45%;">
$$
H_0: \theta = .5 \\
H_1: \theta \neq .5
$$

**$p$-value**

```{r, echo = FALSE}
binom.test(k, N)$p.value
```
</div>
<div style = "float:right; width:45%;">

$$
\begin{split}
H_0&: \theta = .5 \\
H_1&: \theta \sim \mathcal{Beta}(1, 1)
\end{split}
$$

**$BF_{01}$**

```{r, echo = FALSE}
dbeta(0.5, k + 1, N - k + 1)
```  
</div>


## Bayesian prediction
- What is the probability that the next slice of bread will fall butter down?

$$
\begin{align*}
p(y' = 1|y)
&= \int_0^1 p(y' = 1, \theta|y) \mathrm{d}\theta \\
&= \int_0^1 p(y' = 1|\theta)p(\theta|y)\mathrm{d}\theta \\
&= \int_0^1 \theta \cdot p(\theta|y)\mathrm{d}\theta \\
&= \mathbb{E}[\theta|y] = \frac{16}{16 + 5} \approx .76
\end{align*}
$$


## Bayesian prediction
- The prior regularizes and avoids embarrassing inference and prediction
- For $y = 3, n = 3$ the maximum likelihood estimate is $\theta_{\text{MLE}} = 1$ and predicts $p(y' = 1|y)$ with 100% certainty

<span style="color: white"></span>

- This has consequences for a host of more complicated models and is termed *overfitting*

<span style="color: white"></span>

- Bayesian prediction safeguards against this
- Classical methods achieving the same thing are called **penalized likelihood** methods, such as
    - the lasso (which corresponds to Bayesian regression with Laplace priors)
    - ridge regression (which corresponds to Bayesian regression with Gaussian priors)


# Some applications

## Simulator sickness and age
```{r, echo = FALSE}
ss_dat <- read.csv('../data/ss_dat.csv')

ggplot(ss_dat, aes(x = age, y = diff_ssq)) +
  geom_smooth(method = 'lm', se = FALSE, color = 'skyblue') +
  geom_point() +
  xlab('Age') +
  ylab('Simulator Sickness Score (Post - Pre)')
```

## Simulator sickness and age

<center>
<img src="images/simulator-sickness-jasp.png" />
</center>

- What did we actually do? What is our <span style="color: red">statistical model?</span>

## Simulator sickness and age
- Propose as statistical model a multivariate Gaussian distribution for the standardized variable $X = [x_1, x_2]$

$$
f(x|\Sigma) = \frac{1}{\sqrt{2\pi}} |\Sigma|^{-1} \exp \big (\frac{1}{2}x^T\Sigma^{-1}x \big)
$$

```{r}
X <- ss_dat[, c('age', 'diff_ssq')]
Xs <- scale(X)
head(cbind(X, Xs))
```


## Bivariate Gaussian Distribution
```{r, echo = FALSE}
library('mvtnorm')

shinyApp(
  options = list(width = "25%", height = "25%"),
  ui = shinyUI(fluidPage(
    sidebarLayout(
      sidebarPanel(
        tags$head(
          tags$script(src = mathjax_URL, type = 'text/javascript'),
          tags$script("MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});", type = 'text/x-mathjax-config')
    ),
        sliderInput("rho", label = HTML('$$\\rho$$'),
                    min = -1, max = 1, value = 0, step = 0.1)
        ),
      mainPanel(
        plotOutput("mvn_plot", height="500px", width="600px")
        )
      )
   )),
   server = function(input, output) {
      plot_dat <- function(rho) {
        mu <- c(0, 0)
        sigma <- matrix(c(1, rho, rho, 1), nrow = 2)
        X <- expand.grid(x1 = seq(-3, 3, length.out = 300), x2 = seq(-3, 3, length.out = 300))
        d <- cbind(X, prob = dmvnorm(X, mean = mu, sigma = sigma))

        ggplot(d, aes(x = x1, y = x2, z = prob)) +
          geom_contour() +
          coord_fixed(xlim = c(-3, 3), ylim = c(-3, 3), ratio = 1) +
          xlab(TeX('X_1')) +
          ylab(TeX('X_2')) +
          ggtitle(TeX(paste0('$\\rho = $', round(rho, 2)))) +
          theme(plot.title = element_text(hjust = .5))
      }
      
      output$mvn_plot <- renderPlot({
        plot_dat(input$rho)
      })
    }
)
```

## Simulator sickness and age
- The shape of the multivariate Gaussian is determined by the correlation matrix

$$
\Sigma = \begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix}
$$
- We want to compare

$$
\begin{split}
H_0&: \rho = 0 \\
H_1&: \rho \sim \text{Beta}(1, 1)
\end{split}
$$


- For $M_0$, this is just the simple likelihood for $\rho = 0$

$$
\begin{split}
p(d|M_0) &= \frac{1}{\sqrt{2\pi}} \exp \big (-\frac{1}{2}x^T\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} ^{-1}x \big) \\[1ex]
         &\approx 8.414\mathrm{e}^{-35}
\end{split}
$$

```{r, echo = FALSE, eval = FALSE}
prod(dmvnorm(Xs, mean = c(0, 0), sigma = matrix(c(1, 0, 0, 1), nrow = 2)))
```

## Simulator sickness and age
- Again, for $M_1$ it's a bit more complicated as it involves integration over $\rho$

$$
\begin{split}
p(d|M_1) &= \int_{\rho} p(d, \rho|M_1)\mathrm{d}\rho \\[1ex]
         &= \int_{\rho} p(d|M_1, \rho)p(\rho|M_1)\mathrm{d}\rho \\[1ex]
         &= \int_{\rho} f(x;\Sigma)\text{Unif}(-1, 1)\mathrm{d}\rho \\[1ex]
         &= \int_{\rho} \frac{1}{\sqrt{2\pi}} |\Sigma|^{-1} \exp \big (\frac{1}{2}x^T\Sigma^{-1}x \big) \frac{1}{2} \mathrm{d}\rho \\[1ex]
         &\approx \frac{1}{N} \sum_{\rho_i \sim \text{Unif}(-1, 1)} ^N \frac{1}{\sqrt{2\pi}} |\Sigma|^{-1} \exp \big (\frac{1}{2}x^T\begin{pmatrix} 1 & \rho_i \\ \rho_i & 1 \end{pmatrix}^{-1}x \big) \\[1ex]
         &\approx 4.278\mathrm{e}^{-35}
\end{split}
$$

```{r, echo = FALSE, eval = FALSE}
N <- 100000
rho <- runif(N, -1, 1)
f <- function(x, rho) dmvnorm(x, mean = c(0, 0), sigma = matrix(c(1, rho, rho, 1), nrow = 2))

marginal_likelihood <- 0
for (i in seq(N)) {
  rho_i <- rho[i]
  marginal_likelihood <- marginal_likelihood + prod(f(Xs, rho_i))
}

marginal_likelihood / N
```

## Simulator sickness and age
$$
\text{BF}_{01} = \frac{8.414\mathrm{e}^{-35}}{4.278\mathrm{e}^{-35}} = 1.966
$$

<center>
<img src="images/simulator-sickness-jasp.png" />
</center>


## Simulator sickness and age
- The model I suggested is actually wrong (just look at the population distribution of age)
- Moreover, Pearson correlation does not require Gaussian random variables
- It only requires continuous random variables


## Harry Potter Sorting Hat Quiz
![](images/descriptives-big5.png)

## Harry Potter Sorting Hat Quiz
- Previous research has used a t-test comparing, say
    - Mean extraversion of Gryffindor with Mean extraversion of Ravenclaw, Hufflepuff, Slytherin
    - This is not what we want!
    
<span style="color: white"></span>
    
- Using Bayesian inference, we can compare models with order-constraints on the mean
    
$$
\begin{split}
M_0 &: \mu_{\text{G}} = \mu_{\text{S}} = \mu_{\text{R}} = \mu_{\text{H}} \\
M_f &: \mu_{\text{G}}, \, \mu_{\text{S}} , \, \mu_{\text{R}} , \, \mu_{\text{H}} \\
M_r &: \mu_{\text{G}} > (\mu_{\text{S}} , \, \mu_{\text{R}} , \, \mu_{\text{H}})
\end{split}
$$
    

## Markov chain Monte Carlo with People
<center>
<img src="images/MCMCP-trial.png" />
</center>


## Markov chain Monte Carlo with People {.flexbox .vcenter .emphasize}

<span style = "font-size: 2em; margin-top: -500px;">
[Interactive Exploration of Data](https://fdabl.shinyapps.io/MCMCP/)
</span>


## Markov chain Monte Carlo with People
<center>
<img src="images/MCMCP-model.png" />
</center>


# Practicals with JASP

## JASP
- Undergoes rapid development in Eric-Jan Wagenmakers' lab in Amsterdam
- Has over 20 team members coming from psychology, statistics, and computer science
- Its aim is to supersede SPSS and provide Bayesian analysis to the world


## Current analyses
```{r, results = 'asis', echo = FALSE}
analyses <- rbind(
  c('Descriptives', '-', '-'),
  c('One Sample T-Test', 'Yes', 'Yes'),
  c('Paired Samples T-Test', 'Yes', 'Yes'),
  c('Independent Samples T-Test', 'Yes', 'Yes'),
  c('ANOVA', 'Yes', 'Yes'),
  c('ANCOVA', 'Yes', 'Yes'),
  c('Repeated Measures ANOVA', 'Yes', 'Yes'),
  c('Linear Regression', 'Yes', 'Yes'),
  c('Correlation', 'Yes', 'Yes')
)

papaja::apa_table(
  analyses,
  caption = 'Analyses available in JASP 0.8.1.',
  col.names = c('Analysis', 'Bayesian', 'Frequentist')
)
```

## Current analyses
```{r, results = 'asis', echo = FALSE}
analyses <- rbind(
  c('Binomial Test', 'Yes', 'Yes'),
  c('Contingency Tables', 'Yes', 'Yes'),
  c('Log-Linear Regression', 'Yes', 'Yes'),
  c('Summary Stats', 'Yes', '-'),
  c('Reliability Analysis', 'Not yet', 'Yes'),
  c('Principal Component Analysis', 'Not yet', 'Yes'),
  c('Exploratory Factor Analysis', 'Not yet', 'Yes'),
  c('Structural Equation Modeling', 'Not yet', 'Yes')
)

papaja::apa_table(
  analyses,
  caption = 'Analyses available in JASP 0.8.1.',
  col.names = c('Analysis', 'Bayesian', 'Frequentist')
)
```

## Correlation in JASP
<center>
  <img src="images/jasp-correlation.png" />
</center>

## Correlation in JASP
- **1** Load *Presidents.csv* into JASP. Run descriptive statistics and summarize what you find.

<span style="color: white"></span>

- **2** Conduct a classical correlation test.
    - **2a** Interpret the resulting *p*-value and confidence interval.
    - **2b** Are these concepts valid for the current data set? *Hint:* think about the data generating process.
    
<span style="color: white"></span>

- **3** Conduct a Bayesian correlation test.
    - **3a** Estimate the posterior distribution and interpret the Bayes factor.
    - **3b** Does the default prior in JASP make sense?
    - **3c** Run a robustness check. What do you conclude?
    - **3d** Run a sequential analysis. What do you conclude?
    
    
## t-test in JASP
<div style = "float:left; width:45%;">
![](images/jasp-kitchen-rolls.png)
</div>
<div style = "float:right; width:45%;">
![](images/jasp-t-test.png)
</div>


## t-test in JASP
- **1** Load *KitchenRolls.csv* into JASP. Run descriptive statistics and summarize the data graphically.

<span style="color: white"></span>

- **2** Conduct a classical t-test.
    - **2a** Interpret the resulting *p*-value and confidence interval.
    - **2b** Are these concepts valid for the current data set? *Hint:* think about the data generating process.
    
<span style="color: white"></span>
    
- **3** Conduct a Bayesian t-test.
    - **3a** Estimate the posterior distribution and interpret the Bayes factor.
    - **3b** Does the default prior in JASP make sense?
    - **3c** Run a robustness check. What do you conclude?
    - **3d** Run a sequential analysis. What do you conclude?
    
<span style="color: white"></span>

- **4** Conduct a Bayesian one-sided t-test. Is this approach justifiable?

## Wrap-up
<center>
  <img src="images/desiderata.png" />
</center>

# Final thoughts

## We have ...
- seen and compared different statistical philosophies (Fisher, Neyman-Pearson, Bayes)
- looked at the pitfalls in interpreting classical statistical concepts
- utilized probability to quantify uncertainty (murder mystery, science, cancer)
- learned to appreciate statistical models as complexity reducing tools
- reviewed maximum likelihood, sampling distributions, confidence intervals, $p$-values
- seen how Bayes unifies estimation, comparison, and prediction
- used a beautiful, feature-rich, free statistical software to carry out Bayesian computations

## Solidify your learning
- [Exercises](https://github.com/fdabl/Intro-Stats/blob/master/exercises/exercises.pdf) for this workshop
- [A First Lesson in Bayesian Inference](http://lmpp10e-mucesm.srv.mwn.de:3838/felix/BayesLessons/BayesianLesson1.Rmd) by Eric-Jan Wagenmakers
- [Improving your statistical inferences](https://www.coursera.org/learn/statistical-inferences/home/welcome) by Daniel Lakens

## Thanks to
- EJ Wagenmakers and his lab for writing the wittiest exposures to Bayesian statistics and providing user-friendly, open-source, free statistical software
- Richard Morey (and Jeffrey Rouder) for the BayesFactor R package, doing much of the heavy lifting in JASP (see [here](https://bayesfactor.blogspot.de/))
- Michael Franke for allowing me to help him create a course on Bayesian statistics and cognitive modeling (see [here](http://www.sfs.uni-tuebingen.de/~mfranke/bda+cm2015/))
- All the people on Twitter engaging in fruitful discussions, creating an atmosphere of learning unparalleled in University curricula
- All the people who blog about open science and methods